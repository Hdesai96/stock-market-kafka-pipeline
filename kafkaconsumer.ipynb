{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4899d54-4112-4ce7-9e79-7a47c1a3c7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: s3fs in c:\\users\\desai\\anaconda3\\lib\\site-packages (2024.3.1)\n",
      "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in c:\\users\\desai\\anaconda3\\lib\\site-packages (from s3fs) (2.12.3)\n",
      "Requirement already satisfied: fsspec==2024.3.1 in c:\\users\\desai\\anaconda3\\lib\\site-packages (from s3fs) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\desai\\anaconda3\\lib\\site-packages (from s3fs) (3.9.5)\n",
      "Requirement already satisfied: botocore<1.34.70,>=1.34.41 in c:\\users\\desai\\anaconda3\\lib\\site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.34.69)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in c:\\users\\desai\\anaconda3\\lib\\site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.14.1)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in c:\\users\\desai\\anaconda3\\lib\\site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (0.7.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\desai\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\desai\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\desai\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\desai\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\desai\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.9.3)\n",
      "Requirement already satisfied: typing_extensions>=3.7 in c:\\users\\desai\\anaconda3\\lib\\site-packages (from aioitertools<1.0.0,>=0.5.1->aiobotocore<3.0.0,>=2.5.4->s3fs) (4.11.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\desai\\anaconda3\\lib\\site-packages (from botocore<1.34.70,>=1.34.41->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\desai\\anaconda3\\lib\\site-packages (from botocore<1.34.70,>=1.34.41->aiobotocore<3.0.0,>=2.5.4->s3fs) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\desai\\anaconda3\\lib\\site-packages (from botocore<1.34.70,>=1.34.41->aiobotocore<3.0.0,>=2.5.4->s3fs) (2.2.2)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\desai\\anaconda3\\lib\\site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\desai\\anaconda3\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.34.70,>=1.34.41->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cefb4da2-0cd4-4a7b-bad3-d2d160669f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from time import sleep\n",
    "from json import dumps,loads\n",
    "import json\n",
    "from s3fs import S3FileSystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64da0fa6-0867-458c-89d8-fbbd874517c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = KafkaConsumer(\n",
    "    'project1',\n",
    "     bootstrap_servers=['YOUR IP'],\n",
    "    value_deserializer=lambda x: loads(x.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "728ee446-7c90-4ef2-8038-ae00a8381850",
   "metadata": {},
   "outputs": [],
   "source": [
    " #for c in consumer:\n",
    "  # print(c.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e42a68da-9bfc-44cd-b761-877e1cd71ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = S3FileSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe2af7d4-1d4e-4688-b718-f1417f41ae5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m count, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(consumer):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m s3\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3://stockmarketkafka.harshni/stock_market_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(count), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      3\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(i\u001b[38;5;241m.\u001b[39mvalue, file)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\kafka\\consumer\\group.py:1213\u001b[0m, in \u001b[0;36mKafkaConsumer.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1211\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_generator_v2()\n\u001b[0;32m   1212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator)\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   1215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\kafka\\consumer\\group.py:1185\u001b[0m, in \u001b[0;36mKafkaConsumer._message_generator_v2\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_message_generator_v2\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1184\u001b[0m     timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consumer_timeout \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mtime())\n\u001b[1;32m-> 1185\u001b[0m     record_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoll(timeout_ms\u001b[38;5;241m=\u001b[39mtimeout_ms, update_offsets\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1186\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tp, records \u001b[38;5;129;01min\u001b[39;00m six\u001b[38;5;241m.\u001b[39miteritems(record_map):\n\u001b[0;32m   1187\u001b[0m         \u001b[38;5;66;03m# Generators are stateful, and it is possible that the tp / records\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m         \u001b[38;5;66;03m# here may become stale during iteration -- i.e., we seek to a\u001b[39;00m\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;66;03m# different offset, pause consumption, or lose assignment.\u001b[39;00m\n\u001b[0;32m   1190\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m records:\n\u001b[0;32m   1191\u001b[0m             \u001b[38;5;66;03m# is_fetchable(tp) should handle assignment changes and offset\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m             \u001b[38;5;66;03m# resets; for all other changes (e.g., seeks) we'll rely on the\u001b[39;00m\n\u001b[0;32m   1193\u001b[0m             \u001b[38;5;66;03m# outer function destroying the existing iterator/generator\u001b[39;00m\n\u001b[0;32m   1194\u001b[0m             \u001b[38;5;66;03m# via self._iterator = None\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\kafka\\consumer\\group.py:708\u001b[0m, in \u001b[0;36mKafkaConsumer.poll\u001b[1;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[0;32m    706\u001b[0m timer \u001b[38;5;241m=\u001b[39m Timer(timeout_ms)\n\u001b[0;32m    707\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closed:\n\u001b[1;32m--> 708\u001b[0m     records \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll_once(timer, max_records, update_offsets\u001b[38;5;241m=\u001b[39mupdate_offsets)\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m records:\n\u001b[0;32m    710\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m records\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\kafka\\consumer\\group.py:764\u001b[0m, in \u001b[0;36mKafkaConsumer._poll_once\u001b[1;34m(self, timer, max_records, update_offsets)\u001b[0m\n\u001b[0;32m    761\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoll: coordinator needs rejoin; returning early\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[1;32m--> 764\u001b[0m records, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetcher\u001b[38;5;241m.\u001b[39mfetched_records(max_records, update_offsets\u001b[38;5;241m=\u001b[39mupdate_offsets)\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m records\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\kafka\\consumer\\fetcher.py:351\u001b[0m, in \u001b[0;36mFetcher.fetched_records\u001b[1;34m(self, max_records, update_offsets)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m drained:\n\u001b[1;32m--> 351\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;66;03m# To be thrown in the next call of this method\u001b[39;00m\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_in_line_exception_metadata \u001b[38;5;241m=\u001b[39m ExceptionMetadata(fetched_partition, fetched_offset, e)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\kafka\\consumer\\fetcher.py:345\u001b[0m, in \u001b[0;36mFetcher.fetched_records\u001b[1;34m(self, max_records, update_offsets)\u001b[0m\n\u001b[0;32m    343\u001b[0m             fetched_partition \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_partition_records\u001b[38;5;241m.\u001b[39mtopic_partition\n\u001b[0;32m    344\u001b[0m             fetched_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_partition_records\u001b[38;5;241m.\u001b[39mnext_fetch_offset\n\u001b[1;32m--> 345\u001b[0m             records_remaining \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append(drained,\n\u001b[0;32m    346\u001b[0m                                               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_partition_records,\n\u001b[0;32m    347\u001b[0m                                               records_remaining,\n\u001b[0;32m    348\u001b[0m                                               update_offsets)\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m drained:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\kafka\\consumer\\fetcher.py:379\u001b[0m, in \u001b[0;36mFetcher._append\u001b[1;34m(self, drained, part, max_records, update_offsets)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m part\u001b[38;5;241m.\u001b[39mnext_fetch_offset \u001b[38;5;241m==\u001b[39m position\u001b[38;5;241m.\u001b[39moffset:\n\u001b[0;32m    377\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning fetched records at offset \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m for assigned\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    378\u001b[0m               \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m partition \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, position\u001b[38;5;241m.\u001b[39moffset, tp)\n\u001b[1;32m--> 379\u001b[0m     part_records \u001b[38;5;241m=\u001b[39m part\u001b[38;5;241m.\u001b[39mtake(max_records)\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;66;03m# list.extend([]) is a noop, but because drained is a defaultdict\u001b[39;00m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;66;03m# we should avoid initializing the default list unless there are records\u001b[39;00m\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m part_records:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\kafka\\consumer\\fetcher.py:980\u001b[0m, in \u001b[0;36mFetcher.PartitionRecords.take\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    979\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m records:\n\u001b[1;32m--> 980\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;66;03m# To be thrown in the next call of this method\u001b[39;00m\n\u001b[0;32m    982\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_inline_exception \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\kafka\\consumer\\fetcher.py:977\u001b[0m, in \u001b[0;36mFetcher.PartitionRecords.take\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    974\u001b[0m records \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;66;03m# Note that records.extend(iter) will extend partially when exception raised mid-stream\u001b[39;00m\n\u001b[1;32m--> 977\u001b[0m     records\u001b[38;5;241m.\u001b[39mextend(itertools\u001b[38;5;241m.\u001b[39mislice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord_iterator, \u001b[38;5;241m0\u001b[39m, n))\n\u001b[0;32m    978\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    979\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m records:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\kafka\\consumer\\fetcher.py:1037\u001b[0m, in \u001b[0;36mFetcher.PartitionRecords._unpack_records\u001b[1;34m(self, tp, records, key_deserializer, value_deserializer)\u001b[0m\n\u001b[0;32m   1035\u001b[0m value_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(record\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mif\u001b[39;00m record\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1036\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deserialize(key_deserializer, tp\u001b[38;5;241m.\u001b[39mtopic, record\u001b[38;5;241m.\u001b[39mkey)\n\u001b[1;32m-> 1037\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deserialize(value_deserializer, tp\u001b[38;5;241m.\u001b[39mtopic, record\u001b[38;5;241m.\u001b[39mvalue)\n\u001b[0;32m   1038\u001b[0m headers \u001b[38;5;241m=\u001b[39m record\u001b[38;5;241m.\u001b[39mheaders\n\u001b[0;32m   1039\u001b[0m header_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\n\u001b[0;32m   1040\u001b[0m     \u001b[38;5;28mlen\u001b[39m(h_key\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mlen\u001b[39m(h_val) \u001b[38;5;28;01mif\u001b[39;00m h_val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m h_key, h_val \u001b[38;5;129;01min\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m     headers) \u001b[38;5;28;01mif\u001b[39;00m headers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\kafka\\consumer\\fetcher.py:1073\u001b[0m, in \u001b[0;36mFetcher.PartitionRecords._deserialize\u001b[1;34m(self, f, topic, bytes_)\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, Deserializer):\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdeserialize(topic, bytes_)\n\u001b[1;32m-> 1073\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(bytes_)\n",
      "Cell \u001b[1;32mIn[21], line 4\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m consumer \u001b[38;5;241m=\u001b[39m KafkaConsumer(\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproject1\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      3\u001b[0m      bootstrap_servers\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3.145.63.98:9092\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m----> 4\u001b[0m     value_deserializer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: loads(x\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_decoder\u001b[38;5;241m.\u001b[39mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_decode(s, idx\u001b[38;5;241m=\u001b[39m_w(s, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mend())\n\u001b[0;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "for count, i in enumerate(consumer):\n",
    "    with s3.open(\"s3://stockmarketkafka.harshni/stock_market_{}.json\".format(count), 'w') as file:\n",
    "        json.dump(i.value, file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960f38d6-caba-4571-b50c-e0665d2b6f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e74d81b-70db-4890-ad40-283f844e3493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6b3b6e-71d1-4f3c-8263-52e2d6931a27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
